## 语义匹配模型的实现，要点如下：
* 采用dssm模型
* 在句子的语义表达上分别采用TextCNN、LSTM
* 分别实现共享网络和不共享网络两种

## 数据集信息：
原则上只要标注label的pair sentence数据集均可进行；本实的语料来自于： https://www.kaggle.com/c/quora-question-pairs/data   

只使用了train.csv的部分。

## 在实践中可优化的地方：
1）fine train，本实例均采用了word2vec的fine train，好处是：
* 提高模型的收敛速度；
* 增强训练的稳定性，降低波动；
2）word tokenize，注意分词，分词的好坏直接影响数据集的好坏，本实例没有在分词上做过多处理，同时采用英文语料，相对简单。在中文语料上，分词可改进的地方有：
* 清理垃圾词汇，比如异常字符串等；
* 在特定的场景下某些词汇做特定处理，比如地名、虚词、人名在出现较少时为了避免语料语义损失，可将分别归为一类（每类当作一个词处理）；
* 网络结构更精巧的设计，DSSM的诸多改进论文即关注这个点；

二级级联的lstm效果通常较好。

## 是否共享网络
在实现时分别采用了共享网络和不共享网络的方法，事实上不共享网络的评价指标要好一些，但是最终是否共享要看面临的场景或数据集情况。    
* 如果两个输入是可以互换的，比如同一种的语义相似，句子A和B的相似度与句子B和A的相似度是一样的，两者是可以对换的，显然，需要共享网络更加实用。
* 如果两个输入是不可以互换的，比如判断汉语句子A到英语句子B的相似度，这两者通常不可以调换位置，因此，不共享网络更加实用。

在该语料下，最好的做法是把语料qa和qb交换位置后和原来不交换位置的语料一起作为训练集，更加科学，相信共享与不共享的指标评估会是另外一种情况。