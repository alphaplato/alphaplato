# 特征工程概述

## 目录

* 零、概述
* 一、定义
* 二、特征预处理
* 三、特征编码
* 四、特征交叉
* 五、特征选择
* 六、特征评估
* 七、降维
* 八、特征平滑

## 零、概述

特征工程是件特别繁琐的工作，本文意在为特征工程的提供一份执行细则。

## 一、定义

先看段维基百科关于特征工程的定义：“Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated feature learning.
Feature engineering is an informal topic, but it is considered essential in applied machine learning.”   

意思是说，特征工程是利用数据所在领域的相关知识来构建特征，以（最大化）发挥机器学习算法的作用。特征工程是机器学习的应用基础，困难度大、代价高。人工特征的需求可以被自动化特征学习的方法所取代。 

特征工程是一个老生常谈的话题，但却被认为是机器学习应用领域的关键性问题。本文大概包含这么几个内容，首先，会给出一些特征工程常识性的处理方法，其次会着重讲述特征重要性方面的内容。特征重要性方面的内容主要包含两个问题：1.拿到或使用一个特征，如何判断这个特征是有效的？也即特征选择。2.拿到一个数据集，如何评价哪些特征是重要的？也即特征重要性评估。

## 二、特征预处理

特征预处理主要通常只对数据的进一步清洗和归一化。

主要包括以下几个方面：
* 异常者处理
* 缺失值填充
* 归一化，如高斯归一化等。

对于高斯归一化，在深度模型中尤为要重视，比如连续特征的ctr值和counter计数值，两者虽然都属于dense特征，**但是由于在数值量级上差异巨大，深度模型使用时会造成loss剧烈波动甚至模型无法收敛，显而易见，统一量纲下的数据对模型更加友好。**

**值得注意的是，在缺失值处理时，通常是将缺失值补为零，这也意味着在做特征工程是非常忌讳用0表示特征值**，比如，有一个特征字段为is_new，表示这个用户是否新用户，最科学的使用字符（串）’Y‘和’N‘表示，次之的表示方法是是新用户为1，不是为2，未知为0，最糟糕的表示是用1，0两个值点表示是否新用户。

## 三、特征编码

要让算法能够自我学习，还需要让数据变成算法能够读的语言，也就涉及到特征编码。许多机器学习模型都必须将特征表示为实数向量，因为特征值必须与模型权重相乘。特征编码就是将原始数据样本转化成实数向量样本的过程。  

### 1.数值类型
数值类型通常指连续数值特征。整数和浮点数据不需要特殊编码，因为它们可以与数字权重相乘。但在一些机器学习算法中，有时会离散化处理连续型特征。

### 2.分类值类型  
分类特征具有一组离散的可能值。例如，可能有一个名为 street_name 的特征，其中的选项包括：
> {'Charleston Road', 'North Shoreline Boulevard', 'Shorebird Way', 'Rengstorff Avenue'}

可以为模型中的每个分类特征创建一个二元向量来表示这些值，如下所述：  
>* 采用一个几个特征位置作为该特征的表示。  
>* 对于适用于样本的值，将相应向量元素设为 1。  
>* 将所有其他元素设为 0。  
>* 向量的长度等于词汇表中的元素数。当只有一个值为 1 时，这种表示法称为独热编码；当有多个值为 1 时，这种表示法称为多热编码。  

### 3.稀疏型数据

假设数据集中有 100 万个不同的街道名称，您希望将其包含为 street_name 的值。如果直接创建一个包含 100 万个元素的二元向量，其中只有 1 或 2 个元素为 ture，则是一种非常低效的表示法，在处理这些向量时会占用大量的存储空间并耗费很长的计算时间。在这种情况下，一种常用的方法是使用稀疏表示法，其中仅存储非零值。在稀疏表示法中，仍然为每个特征值学习独立的模型权重。

稀疏表示法 (sparse representation)
一种张量表示法，仅存储非零元素。

例如，英语中包含约一百万个单词。表示一个英语句子中所用单词的数量，考虑以下两种方式：

* 要采用密集表示法来表示此句子，则必须为所有一百万个单元格设置一个整数，然后在大部分单元格中放入 0，在少数单元格中放入一个非常小的整数。
* 要采用稀疏表示法来表示此句子，则仅存储象征句子中实际存在的单词的单元格。因此，如果句子只包含 20 个独一无二的单词，那么该句子的稀疏表示法将仅在 20 个单元格中存储一个整数。

比较典型的数据格式，如libsvm。

## 四、特征交叉

特征交叉部分见[feature_cross.md](https://github.com/alphaplato/alphaplato/blob/master/FeatureEngineering/feature_cross.md)。

## 五、特征选择

特征选择有助于最大化特征相关性，同时降低非相关性，从而增加构建较好模型的可能性，并减小模型的整体大小。  

具体来说，有两点：
* 减少特征数量、降维，使模型泛化能力更强，减少过拟合
* 增强对特征和特征值之间的理解

### 1.方差选择 Removing features with low variance

使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。

这里面包含两个方面：
* 数据的量纲，因为特征的量纲不同，方差并不具有可比性，因此，量纲统一是这条准则的前提；否则需要相对比较。
* 假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。

### 2.单变量特征选择 Univariate feature selection

单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验等方式对特征进行测试。

这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）；这种方法有许多改进的版本、变种。

#### [1].卡方检验 Chi-Squared Test

经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量:
$$ \chi = \frac{\sum (A-E)^2}{E} $$

具体可以参考[《卡方检验》](https://wiki.mbalib.com/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C)。不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。

特别地：
* 卡方检验特别适用于类别特征与类别target的度量。

#### [2].Pearson相关系数 Pearson Correlation

皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关。Pearson相关性有效的前提是两个变量的变化关系是单调的。

两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差和标准差的商。
$$ \rho = \frac {Cov(X,Y)}{\sigma_X \cdot \sigma_Y} = \frac{E[(X-\mu_X)*(Y-\mu_Y)]}{\sigma_X \cdot \sigma_Y} $$

皮尔森相关系数使用需要注意三点：
* 只能计算连续（数值）变量间的系数，类别变量无法衡量。
* 衡量的是变量间的线性关系。
* 即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）。

#### [3].互信息熵和最大信息系数 Mutual information and maximal information coefficient (MIC)

互信息的数学定义为：
$$ I(X,Y) = \sum_{y \in Y} \sum_{x \in X} p(x,y) \log \frac{p(x,y)}{p(x) \cdot p(y)} $$

互信息又可以等价地表示成：
$ I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) $ 。

其中条件熵的计算方式为：
$$ H(Y|X) = \sum_{x \in X} p(x) \cdot H(Y|X=x) $$

互信息表示的是: 互信息越小，两个来自不同事件空间的随机变量彼此之间的关系性越低; 互信息越高，关系性则越高 。

互信息熵的使用需要存在以下两个问题：
* 它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；
* 对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。

因此需要最大信息系数（MIc,Maximal information coefficient），类似于归一化。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。

公式如下：
$$ MIC(X,Y) = \max_{|X| \cdot |Y| < B} \frac{I(X,Y)}{\log_2 (\min(|X|,|Y|))} $$

B通常取数据总量的0.6或者0.55次方。

值得一提的是：
* MIC的统计能力遭到了一些质疑，当零假设不成立时，MIC的统计就会受到影响。在有的数据集上不存在这个问题，但有的数据集上就存在这个问题。

#### [4].距离相关系数 (Distance correlation)

距离相关系数是为了克服Pearson相关系数的弱点而生的。在x和x^2这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。距离相关系数的数值区间为[0，1]。

利用Distance correlation 研究两变量u和v的独立性，即为dcorr(u,v)。当dcorr(u,v)=0时，说明u和v相互独立；dcorr(u,v)越大，说明u和v的相关性越强。设 $ \\{ (u_i,v_i), i=1,2,...,n \\} $ 是总体(u,v)的随机样本，Szekely定义u和v的dcorr(u,v)为：

$$ dcorr(u,v) = frac{dcov(u,v)}{\sqrt{dcov(u,u)} \sqrt{dcov(u,v)}} $$

其中：
$$ dcov(u,u) = S_1+S_2 - 2S_3 $$
其中：
$$ S_1 = \frac{1}{n^2} \sum_{i=1}^n { \sum_{j=1}^n {Norm(u_i,u_j)\cdot Norm(v_i,v_j)}} $$
$$ S_2 = \frac{1}{n^2} \sum_{i=1}^n {Norm(u_i,u_j)} \frac{1}{n^2} \sum_{j=1}^n {Norm(v_i,v_j)} $$
$$ S_3 = \frac{1}{n^3} \sum_{i=1}^n { \sum_{j=1}^n { \sum_{l=1}^n {Norm(u_i,u_l)\cdot Norm(v_j,v_l)}}}$$
其中：
$$ Norm(u_i,u_j) = ||u_i - u_j||_{d_u} $$

$$ Norm(v_i,v_j) = ||v_i - v_j||_{d_v} $$ 

### 3、两种顶层特征选择算法

#### [1]、稳定性选择 Stability selection

稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。

#### [2]、递归特征消除 Recursive feature elimination (RFE)

递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一边，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。

RFE的稳定性很大程度上取决于在迭代的时候底层用哪种模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。

## 六、特征评估

特征重要性是模型可解释性的依据，在模型稳定性的控制方面具有重要意义，是建立特征体系的必不可少的一环。常用的特征重要性评估方法主要为树模型评估，如XGBoost和随机森林。

### 1、特征覆盖率

比方说要构造某个年龄的特征，那么这些用户中具有年龄特征的比例是多少就是一个关键的指标。如果覆盖率低，那么最后做出的特征可以影响的用户数量就会有限制。如果覆盖率高，那么年龄特征做得好，对最后的模型训练结果都会有一个明显的提升。
  
因此，通过对样本的统计，得出数据集样本的特征覆盖率的排序，对模型及数据具有重要指导意义。

### 2、XGBoost评估

xgboost是目前较为常用的树模型工具，其具备丰富的语言接口，并提供五种特征重要性的衡量计算：
* weight: 该特征被选为分裂特征的次数。
* gain: 该特征的带来平均增益(有多棵树)。
* cover: 该特征对每棵树的平均覆盖率(等于total_cover/weight)。
* total_gain: 该特征的带来总增益。
* total_cover: 该特征对每棵树的覆盖率。

值得注意的是，XGBoost对确实特征值的处理：XGBoost在缺失值的处理时，将其看与稀疏矩阵的处理看作一样。在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销。在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。


### 2、随机森林评估

随机森林具有准确率高、鲁棒性好、易于使用等优点，这使得它成为了目前最流行的机器学习算法之一。随机森林提供了两种特征选择的方法：平均不纯度减少（mean decrease impurity）和平均精确率减少（mean decrease accuracy）。

a.平均不纯度减少（mean decrease impurity）

随机森林由多个决策树构成。决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。  
这里的纯度等价于代价增益gain。

b.平均精确率减少 Mean decrease accuracy
另一种常用的特征选择方法就是直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。


## 七、降维

当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。

### 1、主成分分析法（PCA）

### 2、线性判别分析法（LDA）

## 八、特征平滑
平滑：对反馈ctr特征进行平滑处理，避免因数据稀疏性，对整体点击反馈ctr的影响。    
用途：对点击反馈ctr特征进行平滑处理，避免因数据稀疏性，对整体点击反馈ctr的影响。

为什么要对特征值进行平滑处理
广告点击率CTR是度量一个用户对于一个广告的行为的最好的度量方法，广告点击率可以定义为：对于一个广告的被点击(click)的次数于被展示(impression)的次数的比值。

CTR=click/impression

在计算CTR时，由于数据的稀疏性，利用上述的计算方法得到的CTR通常具有较大的偏差，这样的偏差主要表现在如下的两种情况：

例如展示impression的次数很小，如1次，其中，点击的次数也很小(这里的很小是指数值很小)，如1，按照上述的CTR的计算方法，其CTR为1，此时的点击率就被我们估计高了；
例如展示的次数很大，但是点击的次数很小，此时，利用上述的方法求得的CTR就会比实际的CTR要小得多。
出现上述两种现象的主要原因是我们对分子impression和分母click的估计不准确引起的，部分原因可能是曝光不足等等，对于这样的问题，我们可以通过相关的一些广告的展示和点击数据对CTR的公式进行平滑处理。

如何对特征值进行平滑处理：引入Beta（α，β）作为先验概率
关于这个问题雅虎出了个paper，能很好地解决这个问题，是用贝叶斯学派的方法的。

首先两个假设。

假设一，所有的广告有一个自身的ctr，这些ctr服从一个Beta（α，β）分布。

假设二，对于某一广告，给定展示次数时和它自身的ctr，它的点击次数服从一个伯努利分布 Binomial(I, ctr)。

如果用r表示点击率，I表示展示，C表示点击。这两个假设可以用下面的数学表示。

根据这两个假设，假如已经有了很多个广告投放数据，I1,C1,I2,C2……In,Cn，就可以根据这些投放数据列出似然函数

求解这个极大似然问题，得到两个参数alpha（α）和beta（β），然后每个广告的点击率就可以利用这两个参数计算后验ctr（平滑ctr）了，公式如下：

r=(C + alpha) / (I + alpha + beta)

有了这个技术，上面说的那些交叉特征的特征值都用这个技术平滑了一下，就避免了出现1.0或者正无穷这种这么病态的特征值，因为一般都是很小的特征值。

事实证明，平滑技术很有效。

## 参考资料：

[1].[Feature_engineering](https://en.wikipedia.org/wiki/Feature_engineering)  
[2].[谷歌机器学习](https://developers.google.com/machine-learning/crash-course/representation/3、feature-engineering?hl=zh-cn)  
[3].[卡方检测](https://wiki.mbalib.com/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C)  
[4].[皮尔逊积矩相关系数](https://zh.wikipedia.org/wiki/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%A7%AF%E7%9F%A9%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0)  
[5].[Machine learning how to choose features](https://bzdww.com/article/63827/)  
[6].[互信息熵](https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF)  
[7].[机器学习如何选择特征](https://zhuanlan.zhihu.com/p/38522541)    
[8].[特征工程到底是什么](https://www.zhihu.com/question/29316149)  
[9].[XGBoost官方文档](https://xgboost.readthedocs.io/en/latest/python/python_api.html)  
[10].[xgboost特征重要性指标: weight, gain, cover](https://blog.csdn.net/sujinhehehe/article/details/84201415)  
[11].[主成分分析](https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)
