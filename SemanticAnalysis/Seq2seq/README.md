## seq2seq（深度生成）模型的实现，要点如下：

## 数据集信息：
两种不同语言对数据；本实的语料来自于：http://www.manythings.org/anki/

## 在实践中可优化的地方：
1）fine train，本实例均采用了word2vec的fine train，好处是：
* 提高模型的收敛速度；
* 增强训练的稳定性，降低波动；
2）word tokenize，注意分词，分词的好坏直接影响数据集的好坏，本实例没有在分词上做过多处理，同时采用英文语料，相对简单。在中文语料上，分词可改进的地方有：
* 清理垃圾词汇，比如异常字符串等；
* 在特定的场景下某些词汇做特定处理，比如地名、虚词、人名在出现较少时为了避免语料语义损失，可将分别归为一类（每类当作一个词处理）；
* 网络结构更精巧的设计，诸多改进论文即关注这个点；
